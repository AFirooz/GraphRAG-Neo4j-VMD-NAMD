{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8179d327",
   "metadata": {},
   "source": [
    "# <span style='color:Tomato;'>Load Env Variables</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3c5816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory: /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import utils\n",
    "\n",
    "# Add the modules directory to the Python path if needed\n",
    "# sys.path.append(os.path.abspath(\"./modules\"))\n",
    "\n",
    "# load variables into env\n",
    "root_dir = utils.get_project_root()\n",
    "f = root_dir / \".secrets\" / \".env\"\n",
    "assert f.exists(), f\"File not found: {f}\"\n",
    "dotenv.load_dotenv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b8b91",
   "metadata": {},
   "source": [
    "# <span style='color:Tomato;'>Process PDFs</span>\n",
    "\n",
    "We'll use Langchain `PyMuPDF4LLM` to load the PDF files into LangChain documents.\n",
    "\n",
    "We'll also use LLM to convert images into a summery and extract its data.\n",
    "\n",
    "\n",
    "## <span style='color:Orange;'>Initialization</span>\n",
    "\n",
    "### <span style='color:Khaki;'>Basic Imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import concurrent.futures as cf\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "\n",
    "temp_dir = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5e7cb",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Loading PDF file as LangChain Document</span>\n",
    "\n",
    "> Images will be extracted (to text) using a Multimodal LLM.\n",
    "\n",
    "You can either use `load()` method to do it all at once in memory or inclemently do it using `lazy_load()`.\n",
    "\n",
    "Since our docs are big, we'll use `lazy_load()` to also see the progress.\n",
    "\n",
    "To save time, we will load the docs from a pickle file if previously processed, otherwise process them and save them as a pickle.\n",
    "\n",
    "#### <span style='color:LightGreen;'>Custom Splitting Mode</span>\n",
    "\n",
    "> By default, each page in the PDF is a (LangChain) Document!\n",
    "\n",
    "When loading the PDF file you can split it in two different ways:\n",
    "- By page `mode=\"page\"`\n",
    "- As a single text flow `mode=\"single\"`. In other words, the whole PDF would be **one** LangChain Document. You can specify page delimiter to have the pages in the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6c23482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_filename_len(file_path: Path) -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Update the filename length for the given file path.\n",
    "    :param file_path: The file path to update.\n",
    "    :return: A tuple containing the updated file name and its length.\n",
    "    \"\"\"\n",
    "    file_name = file_path.stem.lower()\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        file_len = len(pdf_doc)\n",
    "    return file_name, file_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'biopython' not found. Fuzzy Searching ...\n",
      "file_path = /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD/data/pdfs/BioPython.pdf\n"
     ]
    }
   ],
   "source": [
    "# pdf file\n",
    "file_path = Path() / \"..\" / \"data\" / \"pdfs\" / \"biopython.pdf\"\n",
    "\n",
    "file_path = file_path.resolve()\n",
    "file_path = utils.fuzzy_find(file_path)\n",
    "\n",
    "file_name, file_len = update_filename_len(file_path)\n",
    "\n",
    "# create directory for pkl files\n",
    "pkl_dir = file_path.parent.parent / \"pkls\"\n",
    "pkl_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"file_path = {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b5274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <span style='color:orangered;'>Warning: Deleting Pages !!!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file_path = /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD/data/pdfs/BioPython.pdf\n",
      "extract_file_path = /tmp/tmpgv9x6tdd/biopython_extract.pdf\n",
      "partial_file_path = /tmp/tmpgv9x6tdd/biopython_partial.pdf\n"
     ]
    }
   ],
   "source": [
    "# if a problem occurs during the loading, use this to delete previously processed pages.\n",
    "# todo: the page numbers are reindexed to zero\n",
    "\n",
    "problematic_pages = [391, 392, 395, 428]\n",
    "range_to_keep = range(391, file_len)  # 391 to 445 (exclusive)\n",
    "\n",
    "if 0:\n",
    "    display(Markdown(\"#### <span style='color:orangered;'>Warning: Deleting Pages !!!</span>\"))\n",
    "    temp_dir = Path(tempfile.mkdtemp()) if not isinstance(temp_dir, Path) else temp_dir\n",
    "    temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with fitz.open(file_path) as doc:\n",
    "        # PART I: extract deleted pages\n",
    "        if len(problematic_pages) > 0:\n",
    "            range_to_keep = list(set(range_to_keep) - set(problematic_pages))  # needed for next part\n",
    "\n",
    "            temp_doc = fitz.open()\n",
    "            for page_number in problematic_pages:\n",
    "                temp_doc.insert_pdf(doc, from_page=page_number, to_page=page_number)\n",
    "\n",
    "            extract_file_path = temp_dir / f\"{file_name}_extract.pdf\"\n",
    "            temp_doc.save(extract_file_path)\n",
    "            temp_doc.close()\n",
    "\n",
    "        # ========================================================\n",
    "        # PART II: extract pages to keep\n",
    "        doc.select(range_to_keep)\n",
    "        partial_file_path = temp_dir / f\"{file_name}_partial.pdf\"\n",
    "        doc.save(partial_file_path)\n",
    "    \n",
    "    print(f\"extract_file_path = {extract_file_path}\")\n",
    "    print(f\"partial_file_path = {partial_file_path}\")\n",
    "\n",
    "print(f\"\\nfile_path = {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the correct pages are extracted\n",
    "# with fitz.open(partial_file_path) as doc:\n",
    "#     print(doc[0].get_textpage().extractText())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b22254",
   "metadata": {},
   "source": [
    "#### <span style='color:LightGreen;'>How the Asynchronous Lazy Loading Loop</span>\n",
    "\n",
    "This code demonstrates an asynchronous lazy loading pattern with a progress bar. Let me explain how it works:\n",
    "\n",
    "\n",
    "##### <span style='color:SkyBlue;'>Key Components</span>\n",
    "\n",
    "1. `alazy_load()` - An asynchronous generator that yields documents one by one\n",
    "2. `async for` - Asynchronous iteration through the generator\n",
    "3. `tqdm.tqdm()` - Progress bar visualization\n",
    "4. Batching logic to process documents in chunks of 100\n",
    "\n",
    "##### <span style='color:SkyBlue;'>How the Async Loop Works</span>\n",
    "\n",
    "```python\n",
    "async for doc in tqdm.tqdm(await loader.alazy_load()):\n",
    "    # Process each document as it becomes available\n",
    "```\n",
    "\n",
    "The `await loader.alazy_load()` returns an asynchronous iterable. The `async for` loop then:\n",
    "\n",
    "1. Asynchronously requests the next document\n",
    "2. Waits for it to be retrieved without blocking the event loop\n",
    "3. Updates the progress bar via `tqdm`\n",
    "4. Processes the document once available\n",
    "\n",
    "The batching logic (collecting 100 pages before processing) allows for more efficient operations on groups of documents rather than one at a time.\n",
    "\n",
    "This pattern is especially useful when loading documents involves network requests or other I/O operations that would otherwise block execution.\n",
    "\n",
    "\n",
    "#### <span style='color:LightGreen;'>Which LLM to use?</span>\n",
    "\n",
    "- `gemma3:4b`: **biggest,** but provide a general understanding of the images.\n",
    "- `granite3.2-vision`: **small,** and fine-tunned for data extraction from images in PDF docs.\n",
    "- `moondream`: **smallest,** but only good for overall description of the image.\n",
    "\n",
    "The Prompt used:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "You are an assistant tasked with summarizing images for retrieval.\n",
    "1. These summaries will be embedded and used to retrieve the raw image.\n",
    "   Give a concise summary of the image that is well optimized for retrieval\n",
    "2. extract all the text from the image. Do not exclude any content from the page.\n",
    "Format answer in markdown without explanatory text and without markdown delimiter ``` at the beginning.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "beac619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pfile_name='biopython_extract' -> 4 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.parsers import LLMImageBlobParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_pymupdf4llm import PyMuPDF4LLMLoader\n",
    "\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "# Use ChatOllama instead of OllamaLLM for compatibility with LLMImageBlobParser\n",
    "\n",
    "\n",
    "pfile_path = extract_file_path\n",
    "extract_images = False\n",
    "\n",
    "pfile_name, pfile_len = update_filename_len(pfile_path)\n",
    "\n",
    "if extract_images:\n",
    "    loader = PyMuPDF4LLMLoader(\n",
    "        pfile_path,\n",
    "        mode=\"page\",\n",
    "        extract_images=True,\n",
    "        images_parser=LLMImageBlobParser(model=ChatOllama(model=\"granite3.2-vision\", max_tokens=1024)),\n",
    "    )\n",
    "    pfile_name = f\"w.img.{pfile_name}\"\n",
    "else:\n",
    "    loader = PyMuPDF4LLMLoader(pfile_path, mode=\"page\")\n",
    "\n",
    "print(f\"{pfile_name=} -> {pfile_len} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a28c9",
   "metadata": {},
   "source": [
    "The nice thing about `lazy_load()`, is that we can stop processing any page and skip it if a problem happen.\n",
    "\n",
    "You can also resume whenever you want or process pages with different config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa7a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docs from pdf. \n",
      "This will take some time (~0 min)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b32242975c4c20abefe3526f396616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded biopython_extract: 4 documents\n"
     ]
    }
   ],
   "source": [
    "if (pkl_dir / f\"docs_{pfile_name}.pkl\").exists():\n",
    "    print(\"Loading docs from pickle\")\n",
    "    with open(pkl_dir / f\"docs_{pfile_name}.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "else:\n",
    "    print(f\"Loading docs from pdf. \\nThis will take some time (~{int(pfile_len / 30)} min)\")  # on average 30 pages per minute\n",
    "\n",
    "    # Option 1: loading small docs\n",
    "    # docs = loader.load()\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option 2: Load documents asynchronously (almost 3x faster)\n",
    "    # assert not extract_images, \"Async loading not supported for image extraction\"\n",
    "    # docs = await loader.aload()\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option 3: lazy load with progress bar\n",
    "    # # todo: make this asynchronous\n",
    "    # docs = []\n",
    "    # for doc in tqdm(loader.lazy_load(), total=pfile_len):\n",
    "    #     docs.append(doc)\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option 4: Load with timeout\n",
    "    # todo: not working properly when timeout is reached\n",
    "    \n",
    "    timeout_seconds = 30\n",
    "    skipped_pages = []\n",
    "    docs = []\n",
    "\n",
    "    def get_next_doc(loader):\n",
    "        return next(loader)\n",
    "\n",
    "    loader_iter = iter(loader.lazy_load())\n",
    "\n",
    "    for i in tqdm(range(pfile_len), total=pfile_len):\n",
    "        with cf.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future = executor.submit(get_next_doc, loader_iter)\n",
    "            try:\n",
    "                doc = future.result(timeout=timeout_seconds)\n",
    "                docs.append(doc)\n",
    "            except cf.TimeoutError:\n",
    "                skipped_pages.append(i)\n",
    "\n",
    "    # pickle save the docs\n",
    "    with open(pkl_dir / f\"docs_{pfile_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 445 documents\n"
     ]
    }
   ],
   "source": [
    "# # merging docs if partially processed\n",
    "# with open(pkl_dir / \"docs_w.img.biopython_part1.pkl\", \"rb\") as f:\n",
    "#     docs0 = pickle.load(f)\n",
    "\n",
    "# with open(pkl_dir / \"docs_w.img.biopython_partial.pkl\", \"rb\") as f:\n",
    "#     docs1 = pickle.load(f)\n",
    "\n",
    "# with open(pkl_dir / \"docs_biopython_extract.pkl\", \"rb\") as f:\n",
    "#     docs2 = pickle.load(f)\n",
    "\n",
    "# docs = docs0 + docs1 + docs2\n",
    "\n",
    "# print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# with open(pkl_dir / f\"docs_w.img.biopython.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(docs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ac331c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[42] Guy St C. Slater, Ewan Birney: “Automated generation of heuristics for biological sequence comparison.” *BMC Bioinformatics* **6** : 31 (2005). `https://doi.org/10.1186/1471-2105-6-31`\n",
       "\n",
       "[43] George W. Snedecor, William G. Cochran: *Statistical methods* . Ames, Iowa: Iowa State University Press\n",
       "(1989).\n",
       "\n",
       "[44] Martin Steinegger, Markus Meier, Milot Mirdita, Harald V¨ohringer, Stephan J. Haunsberger, Johannes\n",
       "S¨oding: “HH-suite3 for fast remote homology detection and deep protein annotation.” *BMC Bioinfor-*\n",
       "*matics* **20** : 473 (2019). `https://doi.org/10.1186/s12859-019-3019-7`\n",
       "\n",
       "[45] Eric Talevich, Brandon M. Invergo, Peter J.A. Cock, Brad A. Chapman: “Bio.Phylo: A unified toolkit\n",
       "for processing, analyzing and visualizing phylogenetic trees in Biopython”. *BMC Bioinformatics* **13** :\n",
       "209 (2012). `https://doi.org/10.1186/1471-2105-13-209`\n",
       "\n",
       "[46] Pablo Tamayo, Donna Slonim, Jill Mesirov, Qing Zhu, Sutisak Kitareewan, Ethan Dmitrovsky, Eric S.\n",
       "Lander, Todd R. Golub: “Interpreting patterns of gene expression with self-organizing maps: Methods\n",
       "and application to hematopoietic differentiation”. *Proceedings of the National Academy of Sciences USA*\n",
       "**96** (6): 2907–2912 (1999). `https://doi.org/10.1073/pnas.96.6.2907`\n",
       "\n",
       "[47] Ian K. Toth, Leighton Pritchard, Paul R. J. Birch: “Comparative genomics reveals what makes an\n",
       "enterobacterial plant pathogen”. *Annual Review of Phytopathology* **44** : 305–336 (2006). `https://doi.`\n",
       "```\n",
       "  org/10.1146/annurev.phyto.44.070505.143444\n",
       "\n",
       "```\n",
       "[48] G´eraldine A. van der Auwera, Jaroslaw E. Kr´ol, Haruo Suzuki, Brian Foster, Rob van Houdt, Celeste\n",
       "J. Brown, Max Mergeay, Eva M. Top: “Plasmids captured in C. metallidurans CH34: defining the\n",
       "PromA family of broad-host-range plasmids”. *Antonie van Leeuwenhoek* **96** (2): 193–204 (2009). `https:`\n",
       "```\n",
       "  //doi.org/10.1007/s10482-009-9316-9\n",
       "\n",
       "```\n",
       "[49] Michael S. Waterman, Mark Eggert: “A new algorithm for best subsequence alignments with application\n",
       "to tRNA-rRNA comparisons”. *Journal of Molecular Biology* **197** (4): 723–728 (1987). `https://doi.`\n",
       "```\n",
       "  org/10.1016/0022-2836(87)90478-5\n",
       "\n",
       "```\n",
       "[50] Ziheng Yang and Rasmus Nielsen: “Estimating synonymous and nonsynonymous substitution rates\n",
       "under realistic evolutionary models“. *Molecular Biology and Evolution* **17** (1): 32–43 (2000). `https:`\n",
       "```\n",
       "  //doi.org/10.1093/oxfordjournals.molbev.a026236\n",
       "\n",
       "```\n",
       "[51] Ka Yee Yeung, Walter L. Ruzzo: “Principal Component Analysis for clustering gene expression data”.\n",
       "*Bioinformatics* **17** (9): 763–774 (2001). `https://doi.org/10.1093/bioinformatics/17.9.763`\n",
       "\n",
       "444\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "{'producer': 'pdfTeX-1.40.24',\n",
      " 'creator': 'LaTeX with hyperref',\n",
      " 'creationdate': '2024-01-10T10:15:49+00:00',\n",
      " 'source': '/tmp/tmpgv9x6tdd/biopython_partial.pdf',\n",
      " 'file_path': '/tmp/tmpgv9x6tdd/biopython_partial.pdf',\n",
      " 'total_pages': 50,\n",
      " 'format': 'PDF 1.5',\n",
      " 'title': '',\n",
      " 'author': '',\n",
      " 'subject': '',\n",
      " 'keywords': '',\n",
      " 'moddate': '2024-01-10T10:15:49+00:00',\n",
      " 'trapped': '',\n",
      " 'modDate': 'D:20240110101549Z',\n",
      " 'creationDate': 'D:20240110101549Z',\n",
      " 'page': 49}\n"
     ]
    }
   ],
   "source": [
    "temp = docs[-5]\n",
    "display(Markdown(temp.page_content))\n",
    "print('-'*50)\n",
    "pprint.pp(temp.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755813f9",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>initializing the graph database</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab52954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "# graph = Neo4jGraph()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
